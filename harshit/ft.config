{
  "data": {
    "input_file": "../assets/cot/data/CoT_collection_en.json",
    "train_ratio": 0.90,
    "val_ratio": 0.10,
    "seed": 42
  },
  
  "model": {
    "base_model": "../hf_models/models--Qwen--Qwen2.5-14B-Instruct/snapshots/cf98f3b3bbb457ad9e2bb7baf9a0125b6b88caa8/",
    "max_seq_length": 16384,
    "load_in_4bit": false,
    "device": "0",
    
    "lora": {
      "r": 64,
      "lora_alpha": 128,
      "lora_dropout": 0,
      "bias": "none",
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
      ],
      "use_gradient_checkpointing": "unsloth",
      "random_state": 3407,
      "use_rslora": false,
      "loftq_config": null
    }
  },
  
  "training": {
    "output_dir": "../hf_models/models--Qwen--Qwen2.5-14B-Instruct/snapshots/big_cot_training/",
    "per_device_train_batch_size": 32,
    "per_device_eval_batch_size": 32,
    "gradient_accumulation_steps": 2,
    "warmup_steps": 20,
    "num_train_epochs": 5,
    "learning_rate": 2e-4,
    "fp16": false,
    "bf16": true,
    "logging_steps": 10,
    "optim": "adamw_torch",
    "weight_decay": 0.01,
    "lr_scheduler_type": "cosine",
    "seed": 3407,
    "save_strategy": "epoch",
    "eval_strategy": "epoch",
    "max_seq_length": 16384,
    "dataset_text_field": "text",
    "packing": true,
    "save_strategy": "steps",
    "save_steps": 50,
    "save_total_limit": 2
  }
}